{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "mount_file_id": "1JekEOj2zvvmJA2leXqgj2P9TY_FguuNS",
      "authorship_tag": "ABX9TyP5juFss5UgjuHHgEZrQCBY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cralji/Deep_Krein_RF/blob/main/Krein_Cat_dogs_Keras_Tuner.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preliminares"
      ],
      "metadata": {
        "id": "YIFZ4ESMB7pf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Intalar Paquetes Necesarios"
      ],
      "metadata": {
        "id": "gPy4mk90B_EY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install --upgrade keras==2.15.0\n",
        "!pip install -U git+https://github.com/UN-GCPDS/python-gcpds.image_segmentation.git >> /tmp/null"
      ],
      "metadata": {
        "id": "_pXd5huHC3OQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install keras-tuner"
      ],
      "metadata": {
        "id": "-JJYyPmf5TXZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kwDgI4zIx-pe"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!git clone https://github.com/cralji/Deep_Krein_RF.git"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Incorporando Repo al PATH"
      ],
      "metadata": {
        "id": "tFYqEkVnCDHq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "sys.path.append('/content/Deep_Krein_RF')\n",
        "\n",
        "os.chdir('/content/Deep_Krein_RF')"
      ],
      "metadata": {
        "id": "SQPGWH4YzESQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lKg77UUIzKan"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cambiando KErnel del proceso Gaussiado de la OptimizaciÃ³n Bayesiana dek KerasTuner a un Matern$_{\\nu=\\frac{1}{2}}$"
      ],
      "metadata": {
        "id": "FlHr911SB6ah"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import keras_tuner\n",
        "import os\n",
        "\n",
        "# Get the directory where keras_tuner is installed\n",
        "keras_tuner_dir = os.path.dirname(keras_tuner.__file__)\n",
        "\n",
        "# Construct the path to bayesian.py\n",
        "bayesian_path = os.path.join(keras_tuner_dir, 'src', 'tuners', 'bayesian.py')\n",
        "\n",
        "with open(\"cambio_matern_12.txt\", \"r\") as f:\n",
        "  new_content = f.read()\n",
        "  # print(new_content)\n",
        "\n",
        "with open(bayesian_path, 'w') as file:\n",
        "  file.write(new_content)\n",
        "\n",
        "# print(new_content)"
      ],
      "metadata": {
        "id": "ByrH_-wFzY2g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import keras_tuner\n",
        "import os\n",
        "\n",
        "# Get the directory where keras_tuner is installed\n",
        "keras_tuner_dir = os.path.dirname(keras_tuner.__file__)\n",
        "\n",
        "# Construct the path to bayesian.py\n",
        "bayesian_path = os.path.join(keras_tuner_dir, 'src', 'tuners', 'bayesian.py')\n",
        "\n",
        "print(bayesian_path)"
      ],
      "metadata": {
        "id": "qD7AAQkq40TP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Proceso"
      ],
      "metadata": {
        "id": "LmTF9N56ChJI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cargue Librerias"
      ],
      "metadata": {
        "id": "F9sT5SiLCjon"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from krein_functions import *\n",
        "from models import krein_rff_unet\n",
        "from metrics import DiceCoefficient\n",
        "from losses import WeightedDiceLoss\n",
        "from callbacks import MetricsCallback\n",
        "\n",
        "# General Libraries\n",
        "import time\n",
        "import shutil\n",
        "import random\n",
        "import warnings\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from enum import auto, Enum\n",
        "from functools import partial\n",
        "from datetime import datetime\n",
        "\n",
        "# Image Processing Libraries\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.axes import Axes\n",
        "from matplotlib.cm import coolwarm\n",
        "\n",
        "# Deep Learning Libraries\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras_tuner import Objective\n",
        "from keras_tuner import HyperModel\n",
        "import tensorflow.keras.backend as K\n",
        "from keras.layers import Layer, Activation\n",
        "from keras_tuner import BayesianOptimization\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import Callback\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.utils import get_custom_objects\n",
        "from keras_tuner.engine.hyperparameters import HyperParameters\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Machine Learning Libraries - Sklearn\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.gaussian_process import GaussianProcessRegressor\n",
        "from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C, DotProduct, ExpSineSquared, Matern\n",
        "\n",
        "# Deep Learning Libraries - GCPDS\n",
        "from gcpds.image_segmentation.datasets.segmentation import OxfordIiitPet\n",
        "\n",
        "# Deep Learning Libraries - TensorFlow specific\n",
        "# from tensorflow.keras.losses import Loss\n",
        "# from tensorflow.keras.metrics import Metric\n",
        "from tensorflow.keras import Model, layers, regularizers\n",
        "\n",
        "# Other Libraries\n",
        "import gc\n",
        "import json\n",
        "import gdown\n",
        "import itertools\n",
        "from PIL import ImageFont\n",
        "from dataclasses import dataclass\n",
        "from matplotlib.style import available\n",
        "from tensorflow.python.framework.ops import EagerTensor\n",
        "\n",
        "warnings.filterwarnings(\"ignore\") # Disable warnings"
      ],
      "metadata": {
        "id": "rwSeAikCCntf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Dataset"
      ],
      "metadata": {
        "id": "osz_gjb_RtiX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = OxfordIiitPet()\n",
        "train_dataset, val_dataset, test_dataset = dataset()"
      ],
      "metadata": {
        "id": "5tmgTc5GCoKw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(img,mask):\n",
        "    img = tf.image.resize(img,(256,256))\n",
        "    mask = tf.image.resize(mask,(256,256))#Ch 1: Seed, Ch 2: No germinate, Ch 3: germinate\n",
        "    mask = tf.cast(mask>0, tf.float32)\n",
        "    mask = mask[...,1]\n",
        "    mask = mask[...,tf.newaxis]\n",
        "    # mask_ = tf.reduce_sum(mask, axis =[-1], keepdims=True)\n",
        "    back_ground = tf.reduce_sum(mask, axis =[-1], keepdims=True)\n",
        "    back_ground = back_ground == 0\n",
        "    back_ground = tf.cast(back_ground, tf.float32)\n",
        "    mask = tf.concat([mask,back_ground], axis=-1) #Ch 1: No germinate, Ch 2: germinate, Ch 3: Background\n",
        "    return img,mask\n",
        "\n",
        "train = train_dataset.map(lambda x,y,label,id:preprocess(x,y))\n",
        "train = train.batch(64)\n",
        "train = train.cache()\n",
        "\n",
        "\n",
        "val = val_dataset.map(lambda x,y,label,id:preprocess(x,y))\n",
        "val = val.batch(64)\n",
        "val = val.cache()"
      ],
      "metadata": {
        "id": "maH9uuF-Rr9e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model(hp:HyperParameters):\n",
        "\n",
        "  # parameters\n",
        "  learning_rate = hp.Float('learning_rate', min_value=1e-4, max_value=1e-1, sampling='log')\n",
        "  phi_units = hp.Int('phi_units', min_value=2, max_value=8, step=2)\n",
        "\n",
        "  # Build model\n",
        "  model = krein_rff_unet((256,256,3),2,phi_units=phi_units)\n",
        "  optimizer = tf.keras.optimizers.RMSprop(learning_rate = learning_rate)\n",
        "  model.compile(loss = WeightedDiceLoss(weights=[0.5,0.5]),\n",
        "                optimizer=optimizer,\n",
        "                metrics=[DiceCoefficient(name='dice_cat_dog',target_class=0),\n",
        "                         DiceCoefficient(name='dice_background',target_class=1),\n",
        "                         DiceCoefficient(name = 'dice')\n",
        "                        ]\n",
        "                  )\n",
        "  return model"
      ],
      "metadata": {
        "id": "7Cm2R7INgr36"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import Callback\n",
        "\n",
        "class MetricsCallback(Callback):\n",
        "    \"\"\"\n",
        "    Custom Keras callback to log metrics at the end of training epochs.\n",
        "\n",
        "    This callback logs metrics such as loss and accuracy at the end of each epoch\n",
        "    during training. It stores the metrics in a global list `global_metrics` upon\n",
        "    completion of the final epoch.\n",
        "\n",
        "    Parameters:\n",
        "        epochs (int): Total number of epochs for training.\n",
        "\n",
        "    Methods:\n",
        "        on_epoch_end(epoch, logs=None):\n",
        "            Called at the end of each epoch. Logs metrics and stores them in\n",
        "            `global_metrics` upon completion of the final epoch.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, epochs):\n",
        "        \"\"\"\n",
        "        Initializes the MetricsCallback instance.\n",
        "\n",
        "        Parameters:\n",
        "            epochs (int): Total number of epochs for training.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.epochs = epochs\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        \"\"\"\n",
        "        Callback function called at the end of each epoch.\n",
        "\n",
        "        Logs metrics such as loss and accuracy at the end of each epoch.\n",
        "        Stores the metrics in `global_metrics` upon completion of the final epoch.\n",
        "\n",
        "        Parameters:\n",
        "            epoch (int): Current epoch number (0-indexed).\n",
        "            logs (dict): Dictionary containing the metrics to log.\n",
        "                Typically contains keys like 'loss' and 'accuracy'.\n",
        "        \"\"\"\n",
        "        if epoch == self.epochs - 1:\n",
        "            print(f\"Final Epoch {epoch + 1}:\")\n",
        "            metrics_dict = {'epoch': epoch + 1}\n",
        "            for key, value in logs.items():\n",
        "                print(f\"{key}: {value:.4f}\")\n",
        "                metrics_dict[key] = value\n",
        "            global_metrics.append(metrics_dict)"
      ],
      "metadata": {
        "id": "uq5Fycu2n4Kq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 10\n",
        "global_metrics = []\n",
        "callbacks = [MetricsCallback(num_epochs)]"
      ],
      "metadata": {
        "id": "fWoxbG6MUQll"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define HyperModel and tuner\n",
        "tuner = BayesianOptimization(build_model,\n",
        "                             objective=Objective('val_dice',direction='max'),\n",
        "                             max_trials=30,\n",
        "                             executions_per_trial=1,\n",
        "                             directory='./my_dir',\n",
        "                             project_name='loss_dice')"
      ],
      "metadata": {
        "id": "gnepb8r-bMWW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit the tuner\n",
        "tuner.search(train, epochs=num_epochs, validation_data=val, callbacks=callbacks)"
      ],
      "metadata": {
        "id": "xJXXMwyWbehb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tuner.oracle.gpr.kernel_"
      ],
      "metadata": {
        "id": "NRF9El5VunlR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gp = tuner.oracle.gpr # tuner.oracle._gpr_trained()\n",
        "print(f\"Kernel parameters before training: {gp.kernel}\\nKernel parameters after training: {gp.kernel_}\")"
      ],
      "metadata": {
        "id": "OtQMCh7wl5Bv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trial.hyperparameters.values"
      ],
      "metadata": {
        "id": "pzd_PMQqwd2B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert to a pandas data frame the metrics of each epoch of each trial\n",
        "df_metrics = pd.DataFrame(global_metrics)\n",
        "\n",
        "# List to store the results\n",
        "trial_results = []\n",
        "\n",
        "# Iterate over all trials\n",
        "for trial in tuner.oracle.get_best_trials(num_trials=tuner.oracle.max_trials):\n",
        "    trial_info = {\n",
        "        'trial_id': trial.trial_id,\n",
        "        'hyperparameters': trial.hyperparameters.values,\n",
        "        # 'loss': trial.metrics.get_best_value('val_loss')\n",
        "    }\n",
        "    trial_results.append(trial_info)\n",
        "\n",
        "# Convert the results list to a DataFrame\n",
        "df_trials = pd.DataFrame(trial_results)\n",
        "\n",
        "# Apply pd.json_normalize to expand the dictionary into new columns\n",
        "df_hyperparameters = pd.json_normalize(df_trials['hyperparameters'])\n",
        "\n",
        "# Combine the original DataFrame with the new hyperparameter columns\n",
        "# hp_df = pd.concat([df_trials['trial_id'], df_hyperparameters, df_trials['loss'], df_metrics['dice']], axis=1)\n",
        "hp_df = pd.concat([df_trials['trial_id'], df_hyperparameters, df_metrics['dice']], axis=1)\n",
        "\n",
        "\n",
        "# Display the combined DataFrame\n",
        "hp_df = hp_df.sort_values(by='trial_id')\n",
        "hp_df.to_json('hp_df.json')\n",
        "hp_df"
      ],
      "metadata": {
        "id": "N4FSlFfpl8IT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plots the distributions\n",
        "plt.figure(figsize=(12, 8))\n",
        "for i, column in enumerate(hp_df.columns[1:]):\n",
        "    plt.subplot(2, 2, i+1)\n",
        "    sns.histplot(hp_df[column], kde=True)\n",
        "    plt.title(column)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "_8KQAQ3ImAoJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "columns_to_log_scale = ['learning_rate']\n",
        "# Plot hyperparameters values vs loss value\n",
        "fig,axes = plt.subplots(1, 2, figsize=(10, 6))\n",
        "\n",
        "# Iterate over each subplot and the corresponding scatterplot is created.\n",
        "for (column, ax) in zip(hp_df.iloc[:, 1:4], axes.flatten()):\n",
        "    sns.scatterplot(x=hp_df[column], y=hp_df['loss'], hue=hp_df['loss'], palette='viridis', ax=ax, legend=False)\n",
        "    ax.set_xlabel(column)\n",
        "    if hp_df[column].name in columns_to_log_scale:\n",
        "        ax.set_xscale('log')\n",
        "    ax.set_ylabel('Loss value')\n",
        "    ax.set_title(f'{column} vs Loss value')\n",
        "\n",
        "# Adjust the design and show the figure\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "b_EnU_Y4mMjS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "columns_to_log_scale = ['learning_rate']\n",
        "# Plot hyperparameters values vs loss value\n",
        "fig,axes = plt.subplots(1, 2, figsize=(10, 6))\n",
        "\n",
        "# Iterate over each subplot and the corresponding scatterplot is created.\n",
        "for (column, ax) in zip(hp_df.iloc[:, 1:4], axes.flatten()):\n",
        "    sns.scatterplot(x=hp_df[column], y=hp_df['dice'], hue=hp_df['dice'], palette='viridis', ax=ax, legend=False)\n",
        "    ax.set_xlabel(column)\n",
        "    if hp_df[column].name in columns_to_log_scale:\n",
        "        ax.set_xscale('log')\n",
        "    ax.set_ylabel('DICE value')\n",
        "    ax.set_title(f'{column} vs DICE value')\n",
        "\n",
        "# Adjust the design and show the figure\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "z2xJRTVmmO7S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot a heatmap of the learning rate and q hyperparameters\n",
        "plt.pcolormesh(hp_df[['learning_rate', 'phi_units']].values)\n",
        "plt.colorbar()  # Add a colorbar\n",
        "plt.show()\n",
        "\n",
        "# Plot the loss values over trials\n",
        "plt.plot(hp_df['loss'].values, label='Loss', linestyle='-', color='blue')\n",
        "\n",
        "# Plot the dice metric values over trials\n",
        "plt.plot(hp_df['dice'].values, label='Dice Metric', linestyle='-', color='orange')\n",
        "\n",
        "# Add a legend to differentiate between the loss and dice metric plots\n",
        "plt.legend()\n",
        "\n",
        "# Display the line plots\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "rMfEm_tvmSCC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Call the _vectorize_trials method from the tuner oracle\n",
        "# This method converts the hyperparameter configurations of trials into vector representations (X)\n",
        "# and pairs these vectors with their corresponding performance scores (y)\n",
        "X, y = tuner.oracle._vectorize_trials()\n",
        "\n",
        "# Generate 300 points evenly spaced between the minimum and maximum values of the first feature in X\n",
        "# Add a small buffer (0.00003) to the minimum and maximum to avoid edge effects\n",
        "hp_1 = np.linspace(X[:,0].min() - 0.00003, X[:,0].max() + 0.00003, 300)\n",
        "\n",
        "# Generate 300 points evenly spaced between the minimum and maximum values of the second feature in X\n",
        "# Add a small buffer (0.00003) to the minimum and maximum to avoid edge effects\n",
        "hp_2 = np.linspace(X[:,1].min() - 0.00003, X[:,1].max() + 0.00003, 300)"
      ],
      "metadata": {
        "id": "7J6CGiabmVCp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a meshgrid for the input space\n",
        "Xx, Yy = np.meshgrid(hp_1, hp_2)\n",
        "\n",
        "# Stack the meshgrid arrays to create the input for prediction\n",
        "XY = np.vstack([Xx.ravel(), Yy.ravel()]).T\n",
        "\n",
        "# Make predictions using the Gaussian Process model\n",
        "mean, std = gp.predict(XY, return_std=True)\n",
        "\n",
        "# Reshape the predictions to match the shape of the meshgrid\n",
        "mean = mean.reshape(Xx.shape)\n",
        "std = std.reshape(Xx.shape)"
      ],
      "metadata": {
        "id": "OlGdmh-_mbEL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the results\n",
        "plt.figure(figsize=(16, 7))\n",
        "\n",
        "# Heatmap of the mean prediction\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.title('Mean Prediction')\n",
        "plt.xlabel('Learning rate')\n",
        "plt.ylabel('phi_units')\n",
        "# Plot the heatmap of the mean prediction using contourf\n",
        "heatmap1 = plt.contourf(hp_1, hp_2, mean, cmap='viridis', aspect='auto')  # Adjust the axis scaling\n",
        "plt.colorbar(heatmap1)  # Add a colorbar\n",
        "plt.scatter(X[:, 0], X[:, 1], c='white', s=50, edgecolors='k') # Scatter plot of the data points in white with black edges\n",
        "\n",
        "# Heatmap of the uncertainty (standard deviation)\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.title('Uncertainty (Standard Deviation)')\n",
        "plt.xlabel('Learning rate')\n",
        "plt.ylabel('phi_units')\n",
        "# Plot the heatmap of the standard deviation using contourf\n",
        "heatmap2 = plt.contourf(hp_1, hp_2, std, cmap='viridis', aspect='auto')  # Adjust the axis scaling\n",
        "plt.colorbar(heatmap2)  # Add a colorbar\n",
        "plt.scatter(X[:, 0], X[:, 1], c='white', s=50, edgecolors='k') # Scatter plot of the data points in white with black edges\n",
        "\n",
        "# Adjust the spacing between subplots to prevent overlap\n",
        "plt.tight_layout(pad=3.0)\n",
        "# Display the plots\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Lic8ChUzmgKa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Guardar en Repo GitHub"
      ],
      "metadata": {
        "id": "N5ZB-wcE6drP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# path_token = \"/content/drive/MyDrive/____UTP/tokens/token_colab.py\"\n",
        "# # Lee el token de tu Google Drive\n",
        "# with open(path_token, 'r') as token_file:\n",
        "#     token = token_file.read().strip()\n",
        "\n",
        "# # Configura el comando git para usar el token\n",
        "# os.system(\"git config --global user.email 'craljimenez@utp.edu.co'\")\n",
        "# os.system(\"git config --global user.name 'Cristian Jimenez Colab'\")\n",
        "# os.system(f\"git remote set-url origin https://{token}@github.com/cralji/Deep_Krein_RF.git\")"
      ],
      "metadata": {
        "id": "OCeCVAMbBhQH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Hacer commit y push de los cambios\n",
        "# !git add .\n",
        "# !git commit -m \"Se Corrige paquetes no introducidos en losses.py\"\n",
        "# !git push origin main\n",
        "\n",
        "# # Hacer pull de los cambios remotos\n",
        "# !git pull origin main"
      ],
      "metadata": {
        "id": "ti0HxKth6MkY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !git status\n"
      ],
      "metadata": {
        "id": "KFG5-VoWyBpp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}